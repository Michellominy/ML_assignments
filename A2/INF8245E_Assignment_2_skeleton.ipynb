{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGUexl-F8Jz"
      },
      "source": [
        "## Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMnyaCUtF8J5"
      },
      "source": [
        "## Instructions\n",
        "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "\n",
        "* Parts of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want. Please also note that the **public tests are not exhaustive**, meaning that passing the public tests does not necessarily imply that your implementation is correct. It is recommended to test your implementation in your own ways instead of solely relying on the autograder.\n",
        "\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "\n",
        "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
        "\n",
        "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
        "\n",
        "**Warning:** Throughout the assignment, you will be asked to implement certain algorithms. In the solution you submit, do not simply call a library function which performs the entire algorithm for you, this is forbidden, as it would obviously defeat the purpose.  For example, if you were asked to implement the kNN classifier algorithm, do not simply call `sklearn.neighbors.KNeighborsClassifier` on the given data and submit that as your solution.\n",
        "\n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1) Submit a `.ipynb` notebook to the `Assignment 2 - Practial` section on Gradescope.\n",
        "2) Submit a `pdf` version of the notebook to the `Assignment 2 - Analysis` entry.\n",
        "\n",
        "**Note 1**: The only exceptions to the warning above are _Question 1.1_ and _Question 2.2_ in which you're allowed to the use SciPy's functions. Relevant hints are given to you at certain stages of the assignment to help you.\n",
        "\n",
        "**Note 2**: You can choose to submit responses in either English or French.\n",
        "\n",
        "\n",
        "## Context\n",
        "In this assigment, you are asked to:\n",
        "\n",
        "* Work with the image classification dataset. Specifically, you will implement the following three classifiers and compare their performance on two datasets.\n",
        "  * Implement a k-Nearest Neighbour classifier\n",
        "  * Implement a Gaussian Naive Bayes classifier\n",
        "  * Implement a Logisitic Regression classifier\n",
        "\n",
        "* Derive Poission Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3xPe8ECF8J6"
      },
      "source": [
        "The next two cells install and initialize otter-grader. You should run these cells as they are WITHOUT modifying them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.101610Z",
          "start_time": "2023-09-25T14:39:54.701862Z"
        },
        "id": "AsvF6oTBF8J7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: otter-grader in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (5.2.1)\n",
            "Requirement already satisfied: dill in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (0.3.7)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (3.1.2)\n",
            "Requirement already satisfied: nbformat in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (5.9.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (2.1.1)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (6.0.1)\n",
            "Requirement already satisfied: python-on-whales in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (0.65.0)\n",
            "Requirement already satisfied: requests in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (2.31.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (1.14.1)\n",
            "Requirement already satisfied: jupytext in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (1.15.2)\n",
            "Requirement already satisfied: click in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (8.1.7)\n",
            "Requirement already satisfied: fica>=0.3.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (0.3.1)\n",
            "Requirement already satisfied: ipython in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (8.16.0)\n",
            "Requirement already satisfied: astunparse in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (1.6.3)\n",
            "Requirement already satisfied: ipywidgets in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (8.1.1)\n",
            "Requirement already satisfied: ipylab in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (1.0.0)\n",
            "Requirement already satisfied: nbconvert in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from otter-grader) (7.8.0)\n",
            "Requirement already satisfied: docutils in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from fica>=0.3.0->otter-grader) (0.20.1)\n",
            "Requirement already satisfied: sphinx in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from fica>=0.3.0->otter-grader) (7.2.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from astunparse->otter-grader) (0.41.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from astunparse->otter-grader) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from click->otter-grader) (0.4.6)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipywidgets->otter-grader) (0.1.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipywidgets->otter-grader) (5.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.9 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipywidgets->otter-grader) (4.0.9)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipywidgets->otter-grader) (3.0.9)\n",
            "Requirement already satisfied: backcall in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (0.19.0)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (3.0.39)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (2.16.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from ipython->otter-grader) (0.6.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jinja2->otter-grader) (2.1.3)\n",
            "Requirement already satisfied: toml in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupytext->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: markdown-it-py>=1.0.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupytext->otter-grader) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupytext->otter-grader) (0.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (4.12.2)\n",
            "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (5.3.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (0.2.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (0.8.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (23.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbconvert->otter-grader) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbformat->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbformat->otter-grader) (4.19.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pandas->otter-grader) (1.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pandas->otter-grader) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pandas->otter-grader) (2023.3)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.5 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from python-on-whales->otter-grader) (2.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from python-on-whales->otter-grader) (4.66.1)\n",
            "Requirement already satisfied: typer>=0.4.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from python-on-whales->otter-grader) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from python-on-whales->otter-grader) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from requests->otter-grader) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from requests->otter-grader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from requests->otter-grader) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from requests->otter-grader) (2023.7.22)\n",
            "Requirement already satisfied: webencodings in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jedi>=0.16->ipython->otter-grader) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->otter-grader) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.10.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (3.10.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (306)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from markdown-it-py>=1.0.0->jupytext->otter-grader) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from nbclient>=0.5.0->nbconvert->otter-grader) (8.3.1)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->otter-grader) (0.2.8)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pydantic!=2.0.*,<3,>=1.5->python-on-whales->otter-grader) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from pydantic!=2.0.*,<3,>=1.5->python-on-whales->otter-grader) (2.10.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from beautifulsoup4->nbconvert->otter-grader) (2.5)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.6)\n",
            "Requirement already satisfied: snowballstemmer>=2.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from sphinx->fica>=0.3.0->otter-grader) (1.4.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from stack-data->ipython->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from stack-data->ipython->otter-grader) (2.4.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from stack-data->ipython->otter-grader) (0.2.2)\n",
            "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader) (25.1.1)\n",
            "Requirement already satisfied: tornado>=6.2 in c:\\users\\michel\\desktop\\poly\\automne_2023\\inf8245e\\lab\\ml_assignments\\a2\\venv\\lib\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader) (6.3.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'public' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8245e-assignments-2023.git public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.101959Z",
          "start_time": "2023-09-25T14:39:56.870149Z"
        },
        "id": "vt2liECeF8J8"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(colab=True, tests_dir='./public/a2/tests')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjCdiYnWF8J9"
      },
      "source": [
        "The next cells call the necessary libraries for this assignment and contain some helper functions for you to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102068Z",
          "start_time": "2023-09-25T14:39:56.874508Z"
        },
        "id": "jwHdT53SF8J-"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "import typing\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Type\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# NOTE: Do NOT change the order of this import call\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import tensorflow as tf\n",
        "# NOTE: Keras has only been used here to import the dataset, you're\n",
        "# not allowed to use it anywhere else (we haven't either!)\n",
        "\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "rng = np.random.RandomState(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:54:01.812152Z",
          "start_time": "2023-09-25T19:54:01.710583Z"
        },
        "id": "qRV_L0MPF8J-"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "def iterate_samples(batch_size, sample_set, label_set, shuffle=True):\n",
        "    # set random seed reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    order = np.arange(sample_set.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(order)\n",
        "\n",
        "    for i in range(0, sample_set.shape[0], batch_size):\n",
        "        batch_samples = sample_set[order[i : i + batch_size]]\n",
        "        batch_labels = label_set[order[i : i + batch_size]]\n",
        "        yield batch_samples, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "riZ-AzFn1JKZ"
      },
      "source": [
        "Next, we load the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102551Z",
          "start_time": "2023-09-25T14:39:56.896422Z"
        },
        "id": "-dRzmj3p1JKZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images for training: 60000\n",
            "Number of images for testing: 10000\n",
            "Size of  MNIST images: (28, 28)\n"
          ]
        }
      ],
      "source": [
        "# Load  MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Shuffle the training set\n",
        "permuted = np.random.permutation(len(x_train))\n",
        "x_train, y_train = x_train[permuted], y_train[permuted]\n",
        "\n",
        "print(f\"Number of images for training: {x_train.shape[0]}\")\n",
        "print(f\"Number of images for testing: {x_test.shape[0]}\")\n",
        "print(f\"Size of  MNIST images: {x_train[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A_40umzW1JKZ"
      },
      "source": [
        "Normalize the training and test data as following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102735Z",
          "start_time": "2023-09-25T14:39:57.231230Z"
        },
        "id": "b08jGv2E1JKZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training inputs' shape after vectorization: (60000, 784)\n",
            "Testing inputs' shape after vectorization: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# NOTE: We normalize the training data by dividing the input by max value.\n",
        "max_value = np.max(x_train)\n",
        "\n",
        "train_images = x_train / max_value\n",
        "test_images = x_test / max_value\n",
        "\n",
        "# Vectorize the data\n",
        "train_images = train_images.reshape((train_images.shape[0], -1))\n",
        "test_images = test_images.reshape((test_images.shape[0], -1))\n",
        "\n",
        "print(f\"Training inputs' shape after vectorization: {train_images.shape}\")\n",
        "print(f\"Testing inputs' shape after vectorization: {test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "I8QIMNi11JKa"
      },
      "source": [
        "### 1. k-Nearest Neighbour Classification (30 points)\n",
        "\n",
        "In this section, we will be looking at the k-nearest neighbours algorithm for classifiying 10 different categores in the MNIST dataset. In particular, we will learn about the following:\n",
        "\n",
        "1. Implement a function that classifies MNIST using the kNN algorithm.\n",
        "2. Perform a brief cross-validation to get the best k."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Ji_9zLF8KG"
      },
      "source": [
        "**Question 1.1 (6 points):** Implement the distance function `get_distance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T12:36:45.014272Z",
          "start_time": "2023-09-27T12:36:44.924018Z"
        },
        "id": "bvSUaGZNF8KG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_distance(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the Euclidean distance between two arrays\n",
        "\n",
        "    Args:\n",
        "        A (np.ndarray): Numpy array of shape [num_samples_a x num_features]\n",
        "        B (np.ndarray): Numpy array of shape [num_samples_b x num_features]\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Numpy array of shape [num_samples_a x num_samples_b] where\n",
        "                    each column contains the distance between one element in\n",
        "                    matrix_b and all elements in matrix_a\n",
        "    \"\"\"\n",
        "    # NOTE: Depending on your implementation, chances are that you might get out-of-memory errors on Colab. If that is\n",
        "    # the case, look into the documentation of SciPy's cdist function.\n",
        "\n",
        "    return scipy.spatial.distance.cdist(A, B, 'euclidean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t6KgWvoe-QmR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong><pre style='display: inline;'>q1.1</pre></strong> passed! ðŸŒˆ</p>"
            ],
            "text/plain": [
              "q1.1 results: All test cases passed!"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q1.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1cs_Z3u1F8KM"
      },
      "source": [
        "**Question 1.2 (16 points):** Implement `get_k_neighbors` function to the get the labels of the k-nearest neighbours from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.106875Z",
          "start_time": "2023-09-25T14:39:57.373706Z"
        },
        "id": "dsE1loQSF8KN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_k_neighbors(distances: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:\n",
        "    \"\"\"Gets the k nearest labels based on the distances\n",
        "\n",
        "    Args:\n",
        "        distances (np.ndarray): Numpy array of shape num_train_samples x num_test_samples\n",
        "                                containing the Euclidean distances\n",
        "        labels (np.ndarray): Numpy array of shape [num_train_samples, ] containing\n",
        "                                the training labels\n",
        "        k (int): Number of nearest neighbours\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Numpy array of shape [k x num_test_samples] containing the\n",
        "                    training labels of the k nearest neighbours for each test sample\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort the distances in ascending and get the indices of the first \"k\" elements\n",
        "    # HINT: You need to sort the distances in ascending order to get the indices\n",
        "    # of the first \"k\" elements. BUT, you would not need to sort the entire array,\n",
        "    # it would be enough to make sure that the \"k\"-th element is in the correct position!\n",
        "\n",
        "    # NOTE: Since the matrix sizes are huge, it would be impractical to run any sort of a\n",
        "    # loop to get the nearest labels. Think about how you can do it without using loops.\n",
        "    \n",
        "    k_indices = np.argpartition(distances, k, axis=0)[:k]\n",
        "    return labels[k_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fGzRSdAX-QmS"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong><pre style='display: inline;'>q1.2</pre></strong> passed! ðŸ™Œ</p>"
            ],
            "text/plain": [
              "q1.2 results: All test cases passed!"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q1.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oVkSsAO8F8KS"
      },
      "source": [
        "**Question 1.3 (6 points):** Implement the `get_prediction` function that returns the label class that occurs most frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107231Z",
          "start_time": "2023-09-25T14:39:57.391489Z"
        },
        "id": "OgMhRJnLF8KT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_prediction(nearest_labels: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Gets the best prediction, i.e. the label class that occurs most frequently\n",
        "\n",
        "    Args:\n",
        "        nearest_labels (np.ndarray): Numpy array of shape [k x num_test_samples] obtained from the output of the get_k_neighbors function\n",
        "\n",
        "    Returns:\n",
        "        np.array: Numpy array of shape [num_test_samples] containing the best prediction for each test sample\n",
        "    \"\"\"\n",
        "    best_prediction_label = []\n",
        "    for test_sample in nearest_labels.transpose():\n",
        "        unique_labels, unique_counts = np.unique(test_sample, return_counts=True)\n",
        "        best_prediction_label.append(unique_labels[np.argmax(unique_counts)])\n",
        "    \n",
        "    return np.asarray(best_prediction_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mPXPbETj-QmS"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong><pre style='display: inline;'>q1.3</pre></strong> passed! ðŸŒŸ</p>"
            ],
            "text/plain": [
              "q1.3 results: All test cases passed!"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q1.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfY0DUy6F8KW"
      },
      "source": [
        "Now, using these functions, we will run the k-NN classifier on a subset of the MNIST dataset! Particularly, we will use 50,000 samples for training, 10,000 samples for validation, and 10,000 samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107604Z",
          "start_time": "2023-09-25T14:39:57.443151Z"
        },
        "id": "0A3-xsgjF8KX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (50000, 784)\n",
            "Validaton set shape: (10000, 784)\n",
            "Test set shape: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107695Z",
          "start_time": "2023-09-25T14:39:57.446943Z"
        },
        "id": "5FLJ9bIxF8KX"
      },
      "outputs": [],
      "source": [
        "def knn_classifier(training_set: np.ndarray, training_labels: np.ndarray,\n",
        "                  test_set: np.ndarray, test_labels: np.ndarray, k: int) -> float:\n",
        "  \"\"\"\n",
        "  Performs k-nearest neighbour classification\n",
        "\n",
        "  Args:\n",
        "    training_set (np.ndarray): Vectorized training images (shape: [num_train_samples x num_features])\n",
        "    training_labels (np.ndarray): Training labels (shape: [num_train_samples, 1])\n",
        "    test_set (np.ndarray): Vectorized test images (shape: [num_test_samples x num_features])\n",
        "    test_labels (np.ndarray): Test labels (shape: [num_test_samples, 1])\n",
        "    k (int): number of nearest neighbours\n",
        "\n",
        "  Returns:\n",
        "    accuracy (float): the accuracy in %\n",
        "  \"\"\"\n",
        "\n",
        "  dists = get_distance(A=training_set, B=test_set)\n",
        "\n",
        "  nearest_labels = get_k_neighbors(distances=dists, labels=training_labels, k=k)\n",
        "\n",
        "  # from the nearest labels above choose the label classes that occurs most frequently\n",
        "  predictions = get_prediction(nearest_labels)\n",
        "\n",
        "  # calculate and return accuracy of the predicitions\n",
        "  accuracy = (np.equal(predictions, test_labels).sum())/len(test_set) * 100.0\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8upWdQfF8KY"
      },
      "source": [
        "With the kNN classifier defined, how would you choose the best possible *k* value? We will try three different values of _k_ (2, 4 and 6) and evaluate on the validation set. You are asked to report the k-value which obtains the best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:51:20.961024Z",
          "start_time": "2023-09-25T14:39:57.449944Z"
        },
        "id": "KZkObP77F8KZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best validation accuracy of 96.92 % for k=4\n"
          ]
        }
      ],
      "source": [
        "# dictionary to store the k values as keys and the validation accuracies as the values\n",
        "val_accuracy_per_k = {}\n",
        "\n",
        "for k in [2, 4, 6]:\n",
        "    val_accuracy_per_k[k] = knn_classifier(train_set, train_labels, val_set, val_labels, k)\n",
        "\n",
        "best_k = max(val_accuracy_per_k, key=val_accuracy_per_k.get)\n",
        "print(f\"Best validation accuracy of {val_accuracy_per_k[best_k]} % for k={best_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fDk6OHNJF8KZ"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 1.4 (1 point):** Report the best validation accuracy and the corresponding *k* value that achieves it? (1 sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZHRCQAF8Ka"
      },
      "source": [
        "k=4 obtains the best validation accuracy of 96.92 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PIZ9kR81F8Kd"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "#### Reporting the test accuracy (1 points)\n",
        "In this section, you are asked to report the test accuracy for the best-k value obtained in the previous question. You are not required to implement anything here, rather, the points are associated to the test accuracy you will report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-09-25T14:51:20.974116Z"
        },
        "id": "qYupmb_yF8Kd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96.53\n"
          ]
        }
      ],
      "source": [
        "# Now, based on the best value of k, we run the kNN classifier on the test set.\n",
        "test_accuracy = knn_classifier(train_set, train_labels, test_set, test_labels, k=best_k)\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fbB7S0FKF8Ke"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 1.5 (1 point):** Report the test accuracy obtained with the best k-value (1 sentence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mogGCoF8Kf"
      },
      "source": [
        "**Answer 1.5:** The test accuracy with k=4 is 96.53 %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "J94fcP1BF8Ko"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 2.  Classification using Gaussian Naive Bayes (21 points)\n",
        "\n",
        "In this section, we will use the Gaussian Naive Bayes (GNB) classifier on the MNIST dataset. The GNB classifier belongs to the family of probabilistic classifiers based on the application of Bayes' Theorem. The term \"naive\" in naive Bayes classifiers comes from the fact that they have a strong independence assumptions between the features. In particular, it assumes that the value of a particular feature is independent of the value of another feature, _given the class variable_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYLuTaxtF8Kp"
      },
      "source": [
        "Consider the training set $\\{( x^{(1)},y^{(1)} ), \\ldots, ( x^{(N)},y^{(N)} )) \\}$ of N labeled examples, and the input features are $x^{(i)} \\in \\mathbb{R}^n$. Since we are interested in multi-class classification, the label $y$ can take K different values i.e. $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. The GNB model assumes that the **class-conditional densities** are distributed according to a multi-variate Gaussian distribution. In other words, the probability of observing the data $x^{(i)}$ given the class variable (also known as, _likelihood_) is given by a Gaussian distribution as shown below:\n",
        "\n",
        "$$\n",
        "P(x \\mid y=k, \\mu_k, \\Sigma_k) = \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "where $\\mu_k$ denotes the class-specific mean vector and $\\Sigma_k$ denotes the class-specific covariance matrix (meaning that each class has its own mean vector and the covariance matrix). Note that since we have a separate covariance matrix for each class $k$, the covariance matrices are not shared among all the classes.\n",
        "\n",
        "Given the likelihood of the model, we can now calculate the posterior probability, that is, the probability of a label belonging to a particular class given the data $x^{(i)}$, using Bayes' theorem as follows:\n",
        "\n",
        "$$\n",
        "P(y=k \\mid x, \\mu_k, \\Sigma_k) = \\frac{P (x \\mid y=k, \\mu_k, \\Sigma_k) P(y=k)}{\\sum_{c=1}^K P (x \\mid y=c, \\mu_c, \\Sigma_c) P(y=c)}\n",
        "$$\n",
        "where $P(y=k)$ denotes the prior probability of a label belonging to a particular class. The denominator is essentially a normalization constant and is not technically required to be implemented. Observe that it this likelihood $P (x \\mid y=k, \\mu_k, \\Sigma_k)$ that is distributed according to a multi-variate Gaussian distribution given below:\n",
        "\n",
        "$$\n",
        "P(x \\mid y=k, \\mu_k, \\Sigma_k) = \\frac{1}{(2 \\pi)^{K/2} | \\Sigma_k |^{1/2} } \\exp \\left( -\\frac{1}{2} (x - \\mu_k)^{T} \\Sigma_k^{-1} (x - \\mu_k) \\right)\n",
        "$$\n",
        "where $| \\Sigma_k |$ denotes the determinant of the covariance matrix, and $K$ denotes the number of classes. An important note here is that the probabilities are small and in the case of high dimensionality they tend to be very close to zero and result in numerical underflow issues. Therefore, we will be considering the _log_ of the likelihood function instead. Hence, the resulting log-likelihood can be written as:\n",
        "\n",
        "$$\n",
        "\\log P(x \\mid y=k, \\mu_k, \\Sigma_k) = - \\frac{K}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(| \\Sigma_k |) - \\frac{1}{2} (x - \\mu_k)^{T} \\Sigma_k^{-1} (x - \\mu_k)\n",
        "$$\n",
        "\n",
        "Likewise, the log-posterior can be written as:\n",
        "$$\n",
        "\\log P(y=k \\mid x, \\mu_k, \\Sigma_k) \\propto \\log P (x \\mid y=k, \\mu_k, \\Sigma_k) + \\log P(y=k) - \\log (\\textrm{const.})\n",
        "$$\n",
        "Note that we have used the $\\propto$ symbol above which indicates that the log of the posterior probability density for a class can be computed as the sum of the log-likelihood and the log prior probability densities, _upto_ the log of the normalization constant.\n",
        "\n",
        "Now, given the theory, there are two implementation questions in this section. In the first, you are asked to compute the class-specific mean and covariance vectors and the prior probabilities for each class. In the second question, you are required to use these 3 quantities to calculate the posterior probability of each class given the data. While the math and notations given above maybe a bit overwhelming, you are not required to implement the multi-variate Gaussian function. You can use SciPy's function for the same, more details are given in the question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3DT25TChF8Kp"
      },
      "source": [
        "**Question 2.1 (10 points):** Complete the `gnb_fit_classifier` function that fits the GNB classifier on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_fXYXMe_F8Kq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def gnb_fit_classifier(X: np.ndarray, Y: np.ndarray, smoothing: float=1e-3) -> typing.Tuple:\n",
        "    \"\"\"Fits the GNB classifier on the training data\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): numpy array of shape [num_samples x num_features] containing the training data\n",
        "        Y (np.ndarray): numpy array of shape [num_samples, ] containing the training labels\n",
        "        smoothing (float, optional): constant to avoid division by zero. Defaults to 1e-3.\n",
        "\n",
        "    Returns:\n",
        "        prior_probs (typing.List[float]): list of length `num_classes` containing the prior probabilities of the training labels\n",
        "        means (typing.List[np.ndarray]): list of length `num_classes` containing the means of the batch of samples belonging to a particular label\n",
        "                                            shape of each element in the list - (num_features, )\n",
        "        vars (typing.List[np.ndarray]): list o f length `num_classes` containing the variances of the batch of samples belonging to a particular label\n",
        "                                            shape of each element in the list - (num_features, )\n",
        "    \"\"\"\n",
        "\n",
        "    # to set the prior probability of each label by counting the number of times the label appears in\n",
        "    # training data and normalizing it by the total number of training samples.\n",
        "    prior_probs = []\n",
        "\n",
        "    means, vars = [], []\n",
        "\n",
        "    labels = np.unique(Y)\n",
        "    num_classes = len(labels)\n",
        "\n",
        "    total_num_training_sample = len(Y)\n",
        "\n",
        "    \n",
        "    for label in labels:\n",
        "        boolean_Y = [value == label for value in Y]\n",
        "        class_samples = X[boolean_Y]\n",
        "        prior_probs.append(len(class_samples) / total_num_training_sample)\n",
        "        means.append(np.mean(class_samples, axis=0))\n",
        "        vars.append(np.var(class_samples, axis=0) + smoothing)\n",
        "\n",
        "    return prior_probs, means, vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rQmE3USY-QmU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong><pre style='display: inline;'>q2.1</pre></strong> passed! ðŸš€</p>"
            ],
            "text/plain": [
              "q2.1 results: All test cases passed!"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q2.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_0j5wzgdF8Ks"
      },
      "source": [
        "**Question 2.2 (10 points):** Complete the `gnb_predict` function to get the predictions from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OEBy1OnrF8Ks",
        "tags": []
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Michel\\Desktop\\POLY\\automne_2023\\INF8245E\\Lab\\ML_assignments\\A2\\INF8245E_Assignment_2_skeleton.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgnb_predict\u001b[39m(X: np\u001b[39m.\u001b[39mndarray, prior_probs: typing\u001b[39m.\u001b[39mList[np\u001b[39m.\u001b[39mndarray],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                     means: typing\u001b[39m.\u001b[39mList[np\u001b[39m.\u001b[39mndarray], \u001b[39mvars\u001b[39m: typing\u001b[39m.\u001b[39mList[np\u001b[39m.\u001b[39mndarray], num_classes: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the predictions of all test samples from the GNB classifier\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m        np.ndarray: numpy array of shape (num_samples) containing predictions for each test sample\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X54sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     num_samples, feature_dim \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "def gnb_predict(X: np.ndarray, prior_probs: typing.List[np.ndarray],\n",
        "                    means: typing.List[np.ndarray], vars: typing.List[np.ndarray], num_classes: int) -> np.ndarray:\n",
        "    \"\"\"Computes the predictions of all test samples from the GNB classifier\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): numpy array of shape [num_samples x features] containing vectorized test images\n",
        "        prior_probs (typing.List[float]): list of length `num_classes` containing the prior probabilities of the training labels\n",
        "        means (typing.List[np.ndarray]): list of length `num_classes` containing the means of the batch of samples belonging to a particular label\n",
        "        vars (typing.List[np.ndarray]): list of length `num_classes` containing the variances of the batch of samples belonging to a particular label\n",
        "        num_classes (int): int defining the number of classes\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: numpy array of shape (num_samples) containing predictions for each test sample\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples, feature_dim = X.shape\n",
        "\n",
        "    all_preds = np.zeros((num_samples,  num_classes))\n",
        "\n",
        "    # HINT: Check out SciPy's multivariate normal documentation and\n",
        "    # think about which function to use to prevent underflow issues\n",
        "\n",
        "    ...\n",
        "\n",
        "    # for each prediction in `all_preds`, get the label the label that occurs most frequently\n",
        "    preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oG3MYKDZ-QmU"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'grader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Michel\\Desktop\\POLY\\automne_2023\\INF8245E\\Lab\\ML_assignments\\A2\\INF8245E_Assignment_2_skeleton.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grader\u001b[39m.\u001b[39mcheck(\u001b[39m\"\u001b[39m\u001b[39mq2.2\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'grader' is not defined"
          ]
        }
      ],
      "source": [
        "grader.check(\"q2.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TapwtOGVF8Ku"
      },
      "source": [
        "Now using the functions above let us test the GNB classifer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NkBZKuF8Kv"
      },
      "outputs": [],
      "source": [
        "def gnb_classifier(train_set, train_labels, test_set, test_labels, smoothing=1e-3):\n",
        "\n",
        "  num_classes = len(np.unique(y_train))\n",
        "\n",
        "  prior_probs, means, vars = gnb_fit_classifier(train_set, train_labels, smoothing=1e-3)\n",
        "\n",
        "  preds = gnb_predict(test_set, prior_probs, means, vars, num_classes)\n",
        "\n",
        "  accuracy = np.mean(np.equal(preds, test_labels)) * 100.0\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmZvM2PdF8Kv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (50000, 784)\n",
            "Validaton set shape: (10000, 784)\n",
            "Test set shape: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9E3_LSYVF8Kw"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 2.3 (1 point):** Report the test accuracy obtained by the GNB classifier. (1 sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BD7g4vWUF8Kw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 9.8 %\n"
          ]
        }
      ],
      "source": [
        "# test the model!\n",
        "test_acc = gnb_classifier(train_set, train_labels, test_set, test_labels)\n",
        "print(f\"Test accuracy: {test_acc} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igocddp-F8Kx"
      },
      "source": [
        "**Answer 2.3:** The test accuracy obtained by the GNB classifier is .... %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "midmXGlAF8K1"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 3.  Classification using Logistic Regression (43 points)\n",
        "\n",
        "In this section, you will be using logistic regression for classifying different categories on the same MNIST dataset. In particular, the following are the objectives for this section:\n",
        "\n",
        "1. Understanding logistic regression for multi-class classification problems.\n",
        "2. Learning to derive the gradient of the softmax function and implement the `softmax` function.\n",
        "3. Implementing the gradient updates in the function `compute_gradient`.\n",
        "4. Understanding and implementing the training, validation, testing phases in a standard machine learning training regime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECV_DfuKF8K1"
      },
      "source": [
        "\n",
        "Consider a logistic regression model for classifying the MNIST categories, where we have a training set $\\{( x^{(1)},y^{(1)} ), \\ldots, ( x^{(N)},y^{(N)} )) \\}$ of N labeled examples, and the input features are $x^{(i)} \\in \\mathbb{R}^n$.\n",
        "\n",
        "Since we are interested in multi-class classification, the label $y$ can take K different values i.e. $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. Note that for ease of notation, we start the index of classes from 1, rather than from 0.\n",
        "\n",
        "Now, given a test input $x^{(i)}$, we want our hypothesis to estimate the probability that $P(y=k | x^{(i)})$ for each value of $k = 1, \\ldots , K $, i.e. we want to estimate the probability of the class label taking on each of the K\n",
        "different possible values. Thus, our hypothesis will output a $K - $\n",
        "dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities. Concretely, the hypothesis function (denoted by $z$) for a single input $x^{(i)}$ takes the following form:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    z_{(w, b)}(x^{(i)}) &=\n",
        "        \\begin{bmatrix}\n",
        "           P(y=1 \\mid x^{(i)}; w, b) \\\\\n",
        "           P(y=2 \\mid x^{(i)}; w, b) \\\\\n",
        "           \\vdots \\\\\n",
        "           P(y=K \\mid x^{(i)}; w, b)\n",
        "         \\end{bmatrix}\n",
        "          &= \\frac{1}{\\sum_{j=1}^K \\exp(h^{(i)}_j)}  \n",
        "          \\begin{bmatrix}\n",
        "           \\exp(h^{(i)}_1) \\\\\n",
        "           \\exp(h^{(i)}_2) \\\\\n",
        "           \\vdots \\\\\n",
        "           \\exp(h^{(i)}_K))\n",
        "         \\end{bmatrix}\n",
        "  \\end{aligned},\n",
        "$$\n",
        "\n",
        "where $h^{(i)} = x^{(i)}.w + b$ containing the parameters of the model. Particularly,  $w \\in \\mathbb{R}^{n \\times K}$ denotes the weight matrix, $b \\in \\mathbb{R}^K$ is the bias vector associated with each of the classes. Lastly, notice in the hypothesis function that we have a term of the form $\\frac{\\exp(â‹…)}{\\sum_j \\exp(â‹…)}$, this is called the softmax function and is used frequently in machine learning for multi-class classification problems since it outputs values as probabilities between 0 and 1.\n",
        "\n",
        "\n",
        "We will use the negative log-likelihood loss for training our logistic regression model. As the name suggests, it simply calculates the negative of the log likelihood of the model and is given by:\n",
        "$$\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^N \\log \\hat{y}^{(i)} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\left( \\frac{ e^{h^{(i)}_y} }{ \\sum_{j=1}^K e^{h^{(i)}_j} } \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hLqBZ6V1F8K2"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.1 (10 points)**\n",
        "\n",
        "Now that we have defined our model and the loss function, calculate the derivative of the loss function $L$ w.r.t the weight matrix $w$ and the bias vector $b$. In other words, get the gradient update expressions for $w$ i.e. $\\frac{\\partial L}{\\partial w}$ and $b$ i.e. $\\frac{\\partial L}{\\partial b}$. Make sure the shapes of all matrices involved are consistent as given in the description above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhO3llsnF8K2"
      },
      "source": [
        "**Hints**:\n",
        "\n",
        "1. It might be good to start by calculating the derivative of the softmax function.\n",
        "2. Think about how you can get the derivative of a particular quantity when it is inside a summation. Is there a way to divide the term into sub-terms and then computing the individual gradients?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHS7PUroF8K2"
      },
      "source": [
        "**Answer 3.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "vHUUu9ooF8K4"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "With all the theory in place, we will now implement individual functions at bring them all together at the end to train your logistic regression model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "WmfUK1o9F8K4"
      },
      "source": [
        "**Question 3.2 (2 Points)**\n",
        "Implement the `softmax` function below. As we have seen in the introduction, the softmax function is given by:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\textrm{softmax}(x) = \\frac{\\exp(x)}{ \\sum_{j} \\exp{(x_j)}}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEGz8D6TF8K4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Takes the input and applies the softmax function to it\n",
        "\n",
        "    Args:\n",
        "        x: Numpy array\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: the softmax-ed input\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FSdCvtVf-QmW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.2</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.2 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 14 in test q3.2:\n",
              "          np.testing.assert_allclose(_smx.sum(axis=1), [1,1,1])\n",
              "    AttributeError: NoneType' object has no attribute 'sum</pre>"
            ],
            "text/plain": [
              "q3.2 results:\n",
              "    q3.2 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 14 in test q3.2:\n",
              "              np.testing.assert_allclose(_smx.sum(axis=1), [1,1,1])\n",
              "        AttributeError: NoneType' object has no attribute 'sum"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NRUF0RQcF8K7"
      },
      "source": [
        "**Question 3.3 (3 points)**\n",
        "A skeleton of the logistic regression model is given to you in the `LogisticRegressionModel` class. Implement the `__call__` function that essentially computes the output probability given the batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:06.275535Z",
          "start_time": "2023-09-25T19:55:06.232048Z"
        },
        "id": "foHpJ5jMF8K7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionModel:\n",
        "    def __init__(self, init_weights: np.ndarray) -> None:\n",
        "        num_classes = init_weights.shape[1]\n",
        "        # the weight matrix. Shape = [num_features, num_classes]\n",
        "        self.W = np.copy(init_weights)\n",
        "        # the bias vector. Shape = [num_classes]\n",
        "        self.b = np.zeros((num_classes))\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray :\n",
        "        \"\"\"\n",
        "        Computes the hypothesis function, i.e. the prediction (y_hat) of the logistic regression model\n",
        "\n",
        "        Args:\n",
        "            x: Numpy array of shape [batch_size x num_features] containing input mini-batch of samples\n",
        "\n",
        "        Returns:\n",
        "            x: Numpy array of shape (shape: [batch_size x num_classes]) containing the output class probabilities\n",
        "                after applying the softmax function\n",
        "        \"\"\"\n",
        "        # HINT: Look into the documentation of np.matmul\n",
        "\n",
        "        ...\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3j6KqYXp-QmW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.3</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.3 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 15 in test q3.3:\n",
              "          np.testing.assert_allclose(LR_model(_X).sum(axis=1), [1,1,1,1])\n",
              "    AssertionError: \n",
              "    Not equal to tolerance rtol=1e-07, atol=0\n",
              "\n",
              "    Mismatched elements: 4 / 4 (100%)\n",
              "    Max absolute difference: 3\n",
              "    Max relative difference: 3.\n",
              "     x: array([3, 2, 3, 4])\n",
              "     y: array([1, 1, 1, 1])</pre>"
            ],
            "text/plain": [
              "q3.3 results:\n",
              "    q3.3 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 15 in test q3.3:\n",
              "              np.testing.assert_allclose(LR_model(_X).sum(axis=1), [1,1,1,1])\n",
              "        AssertionError: \n",
              "        Not equal to tolerance rtol=1e-07, atol=0\n",
              "\n",
              "        Mismatched elements: 4 / 4 (100%)\n",
              "        Max absolute difference: 3\n",
              "        Max relative difference: 3.\n",
              "         x: array([3, 2, 3, 4])\n",
              "         y: array([1, 1, 1, 1])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6Gz43GBjF8K9"
      },
      "source": [
        "**Question 3.4 (3 points)**\n",
        "Implement the `nll_loss` function given the predictions and the target labels as defined in the introduction of this section. To recap, the equation for negative log likelihood is given by:\n",
        "\\begin{equation}\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^N \\log \\hat{y}^{(i)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:14.212282Z",
          "start_time": "2023-09-25T19:55:14.171357Z"
        },
        "id": "lbxQXl_zF8K-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def nll_loss(prediction: np.ndarray, target: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes the negative log likelihood loss between the prediction and the target\n",
        "\n",
        "    Args:\n",
        "        prediction: Numpy array of shape [batch size x num_classes]\n",
        "        target: Numpy array of shape  [batch size, ]\n",
        "\n",
        "    Returns:\n",
        "       (float): the negative log likelihood loss\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = prediction.shape[0]\n",
        "\n",
        "    ...\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZZHEly4o-QmW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.4</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.4 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 12 in test q3.4:\n",
              "          np.testing.assert_allclose(nll_loss(prediction,target), 0.6364828379064438)\n",
              "    NameError: name 'loss' is not defined</pre>"
            ],
            "text/plain": [
              "q3.4 results:\n",
              "    q3.4 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 12 in test q3.4:\n",
              "              np.testing.assert_allclose(nll_loss(prediction,target), 0.6364828379064438)\n",
              "        NameError: name 'loss' is not defined"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pmeWQLfsF8LA"
      },
      "source": [
        "**Question 3.5 (6 points)**\n",
        "Using the gradient update expressions that you have derived in Question 3.1, implement the `compute_gradients` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:17.685936Z",
          "start_time": "2023-09-25T19:55:17.625905Z"
        },
        "id": "6fRru-NNF8LA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_gradients(x: np.ndarray, prediction: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the gradient of the loss function w.r.t the parameters\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Numpy array of shape [batch size x num_features]\n",
        "        prediction (np.ndarray): Numpy array of shape [batch size x num_classes]\n",
        "        target (np.ndarray): Numpy array of shape  [batch size, ]\n",
        "\n",
        "    Returns:\n",
        "        grad_W (np.ndarray): Numpy array of shape [num_features x num_classes]\n",
        "                             i.e. same as the weights matrix\n",
        "        grad_b (np.ndarray): Numpy array of shape [num_classes, ]\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    ...\n",
        "\n",
        "    return grad_W, grad_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1bCjUFdw-QmW"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.5</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.5 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 13 in test q3.5:\n",
              "          received_grad_w, received_grad_b = compute_gradients(X, prediction,target)\n",
              "    NameError: name 'grad_W' is not defined</pre>"
            ],
            "text/plain": [
              "q3.5 results:\n",
              "    q3.5 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 13 in test q3.5:\n",
              "              received_grad_w, received_grad_b = compute_gradients(X, prediction,target)\n",
              "        NameError: name 'grad_W' is not defined"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JD8iJ-LMF8LC"
      },
      "source": [
        "**Question 3.6 (6 points)**\n",
        "Validation is one of the most important phases in training machine learning models. This is done so as to evaluate the learning capability of the model by testing it on the samples from the validation set. The procedure is as follows: Given the model and the batch size, iterate through the validation set to compute the loss and accuracy of the model. Note that in the validation phase, we do not compute the gradients.  \n",
        "Now, Implement the `validation` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:20.706884Z",
          "start_time": "2023-09-25T19:55:20.648094Z"
        },
        "id": "pAWobDVCF8LC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validation(model: Type[LogisticRegressionModel], val_set: np.ndarray, val_labels: np.ndarray,\n",
        "                batch_size: int) -> float:\n",
        "    \"\"\"\n",
        "    Performs validation of the given input model\n",
        "\n",
        "    Args:\n",
        "        model (type: class): the model to be validated\n",
        "        val_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        val_labels (np.ndarray): Numpy array of shape [val_size]\n",
        "        batch_size (int): Int defining the batch_size\n",
        "\n",
        "    Returns:\n",
        "        val_loss (float): the validation loss for the entire validation set\n",
        "        val_acc (float): the validation accuracy for the entire validation set\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    sample_count = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch, labels in iterate_samples(batch_size, val_set, val_labels, False):\n",
        "\n",
        "        ...\n",
        "\n",
        "    validation_loss = total_loss / batch_count\n",
        "    validation_acc = correct_preds / sample_count\n",
        "\n",
        "    return validation_loss, validation_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "YBSMSed--QmX"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.6</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.6 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 19 in test q3.6:\n",
              "          validation_loss, validation_accuracy = validation(model,X,target,batch_size)\n",
              "    ZeroDivisionError: float division by zero</pre>"
            ],
            "text/plain": [
              "q3.6 results:\n",
              "    q3.6 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 19 in test q3.6:\n",
              "              validation_loss, validation_accuracy = validation(model,X,target,batch_size)\n",
              "        ZeroDivisionError: float division by zero"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-y4Zz5ydF8LE"
      },
      "source": [
        "**Question 3.7 (10 points)**\n",
        "Next, implement the `train_one_epoch` function below. This function uses combines the functions that you have already implemented above, namely, the `LogisticRegressionModel` class, the `nll_loss`, `compute_gradients`, and the `validation` functions. This function returns all the necessary outputs required for plotting the training and validation curves as shall be seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:24.021257Z",
          "start_time": "2023-09-25T19:55:23.981629Z"
        },
        "id": "H0bmr_iFF8LF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: Type[LogisticRegressionModel],\n",
        "                    train_set: np.ndarray, train_labels: np.ndarray,\n",
        "                    val_set: np.ndarray, val_labels:np.ndarray,\n",
        "                    batch_size: int, learning_rate: float,\n",
        "                    validation_every_x_step: int) -> float:\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch on the entire dataset with the given learning rate and batch size\n",
        "\n",
        "    Args:\n",
        "        model (class): the model used to train\n",
        "        train_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        train_laels (np.ndarray): Numpy array of shape [val_size]\n",
        "        val_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        val_laels (np.ndarray): Numpy array of shape [val_size]\n",
        "        batch_size (int): the batch size to be used to iterate through the dataset\n",
        "        learning_rate (float): the learning rate to be used for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): the number of steps to wait before performing validation\n",
        "\n",
        "    Returns:\n",
        "        train_losses (list): a list of training losses\n",
        "        train_accuracies (list): a list of training accuracies\n",
        "        # train_steps (list): a list of the training batch ids, i.e. each element is the n-th batch of the training set\n",
        "        train_steps (list): a list of the number of training steps. One training step is defined as one forward pass\n",
        "                            (i.e. calculating the loss) AND one backward pass (i.e. calculating the gradients and updating the parameters)\n",
        "                            of a mini-batch of samples through the model\n",
        "        val_losses (list): a list of validation losses\n",
        "        val_accuracies (list): a list of validation accuracies\n",
        "        val_steps (list): a list of the validation steps. One validation step is defined one forward pass of the validaton mini-batch\n",
        "                            samples through the model\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    train_steps = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_steps = []\n",
        "    step_count = 0\n",
        "\n",
        "    # Iterate through the training set and append the corresponding metrics to the list\n",
        "    for x_batch, targets in iterate_samples(batch_size, train_set, train_labels, True):\n",
        "        ...\n",
        "\n",
        "        # perform validation depending on the value of `validation_every_x_step`\n",
        "        if (step_count % validation_every_x_step) == 0 or step_count == 1:\n",
        "            ...\n",
        "\n",
        "            val_steps.append(step_count)\n",
        "\n",
        "    return train_losses, train_accuracies, train_steps, val_losses, val_accuracies, val_steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wq4htBQr-QmX"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3.7</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3.7 - 1</pre> result:</strong></p><pre>    âŒ Test case failed\n",
              "    Error at line 26 in test q3.7:\n",
              "          np.testing.assert_allclose(len(train_losses), 2)\n",
              "    AssertionError: \n",
              "    Not equal to tolerance rtol=1e-07, atol=0\n",
              "\n",
              "    Mismatched elements: 1 / 1 (100%)\n",
              "    Max absolute difference: 2\n",
              "    Max relative difference: 1.\n",
              "     x: array(0)\n",
              "     y: array(2)</pre>"
            ],
            "text/plain": [
              "q3.7 results:\n",
              "    q3.7 - 1 result:\n",
              "        âŒ Test case failed\n",
              "        Error at line 26 in test q3.7:\n",
              "              np.testing.assert_allclose(len(train_losses), 2)\n",
              "        AssertionError: \n",
              "        Not equal to tolerance rtol=1e-07, atol=0\n",
              "\n",
              "        Mismatched elements: 1 / 1 (100%)\n",
              "        Max absolute difference: 2\n",
              "        Max relative difference: 1.\n",
              "         x: array(0)\n",
              "         y: array(2)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grader.check(\"q3.7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMswf-vF8LH"
      },
      "source": [
        "#### Bringing it all together\n",
        "\n",
        "Using all the functions you have implemented above, you will now train a logistic regression model on the  MNIST dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:27.064291Z",
          "start_time": "2023-09-25T19:55:27.020850Z"
        },
        "id": "4XcBKZJKF8LI"
      },
      "outputs": [],
      "source": [
        "def logistic_fit_classifier(num_epochs: int, batch_size: int, learning_rate: float, validation_every_x_step: int, W_initial_weights: float) -> float:\n",
        "    \"\"\"\n",
        "    Trains the logistic regression model\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Number of epochs to train the model for\n",
        "        batch_size (int): Size of the mini-batch\n",
        "        learning_rate (float): Step size for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): Perform validation at every x-th step\n",
        "        W_initial_weights (float): Randomly initialized weight matrix\n",
        "\n",
        "    Returns:\n",
        "        train_loss: list containing training losses at each epoch\n",
        "        train_accuracy: list containing training accuracies at each epoch\n",
        "        train_step:\n",
        "        val_loss: list containing validation losses at each epoch\n",
        "        val_accuracy: list containing validation accuracies at each epoch\n",
        "        val_step:\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    train_step = []\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "    val_step = []\n",
        "    epoch_last_step = 0\n",
        "\n",
        "    model = LogisticRegressionModel(W_initial_weights)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        epoch_train_loss, epoch_train_accuracy, epoch_train_step, \\\n",
        "             epoch_val_loss, epoch_val_accuracy, epoch_val_step = \\\n",
        "                train_one_epoch(model, train_set, train_labels, val_set,\n",
        "                    val_labels, batch_size, learning_rate,\n",
        "                    validation_every_x_step)\n",
        "\n",
        "        train_loss += epoch_train_loss\n",
        "        train_accuracy += epoch_train_accuracy\n",
        "        train_step += [step + epoch_last_step for step in epoch_train_step]\n",
        "\n",
        "        val_loss += epoch_val_loss\n",
        "        val_accuracy += epoch_val_accuracy\n",
        "        val_step += [step + epoch_last_step for step in epoch_val_step]\n",
        "\n",
        "        epoch_last_step = train_step[-1]\n",
        "\n",
        "    return train_loss, train_accuracy, train_step, val_loss, val_accuracy, val_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:43.918415Z",
          "start_time": "2023-09-25T19:55:27.368854Z"
        },
        "id": "7Dxda69iF8LJ"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Michel\\Desktop\\POLY\\automne_2023\\INF8245E\\Lab\\ML_assignments\\A2\\INF8245E_Assignment_2_skeleton.ipynb Cell 74\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m W_initial_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0.5\u001b[39m, \u001b[39m0.1\u001b[39m, (\u001b[39m784\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train a logistic regression model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_loss_bs100, train_accuracy_bs100, train_step_bs100, val_loss_bs100, \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     val_accuracy_bs100, val_step_bs100 \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         logistic_fit_classifier(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         validation_every_x_step\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, W_initial_weights\u001b[39m=\u001b[39;49mW_initial_weights)\n",
            "\u001b[1;32mc:\\Users\\Michel\\Desktop\\POLY\\automne_2023\\INF8245E\\Lab\\ML_assignments\\A2\\INF8245E_Assignment_2_skeleton.ipynb Cell 74\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     val_accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m epoch_val_accuracy\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     val_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [step \u001b[39m+\u001b[39m epoch_last_step \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m epoch_val_step]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     epoch_last_step \u001b[39m=\u001b[39m train_step[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Michel/Desktop/POLY/automne_2023/INF8245E/Lab/ML_assignments/A2/INF8245E_Assignment_2_skeleton.ipynb#Y133sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_loss, train_accuracy, train_step, val_loss, val_accuracy, val_step\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# initiliaze weights from a normal distribution\n",
        "W_initial_weights = np.random.normal(0.5, 0.1, (784, 10))\n",
        "\n",
        "# train a logistic regression model\n",
        "train_loss_bs100, train_accuracy_bs100, train_step_bs100, val_loss_bs100, \\\n",
        "    val_accuracy_bs100, val_step_bs100 = \\\n",
        "        logistic_fit_classifier(num_epochs=4, batch_size=100, learning_rate=0.1,\n",
        "        validation_every_x_step=10, W_initial_weights=W_initial_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cysgKkacF8LL"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.8 (2 points):** As in the previous section, you are asked to plot the training and validation accuracy curves (points are associated to the type of the curves you report)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:44.058115Z",
          "start_time": "2023-09-25T19:55:43.919155Z"
        },
        "deletable": false,
        "editable": false,
        "id": "QrLwV7IlF8LL"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_step_bs100, train_accuracy_bs100, label='train_accuracy')\n",
        "plt.plot(val_step_bs100, val_accuracy_bs100, label='val_accuracy')\n",
        "plt.xlabel(\"Num Steps\", fontsize=14)\n",
        "plt.ylabel(\"Test Acc.\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "u0PqbT0H-QmY"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "We will be using a batch size of 100 and a learning rate of 0.1 to evaluate the logistic regression classifier on the test set. Ideally, extensive hyperparamter tuning must be done on the validation set to choose the best possible hyperparameter configurations for your classifier. However, one of your objectives in the next assignment will be hyperparameter tuning, hence we are giving you the best hyperparameters in this assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:44.108871Z",
          "start_time": "2023-09-25T19:55:44.050912Z"
        },
        "id": "qIBZoqUyF8LL"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO DEFINE THE FUNCTION FOR TESTING THE MODEL\n",
        "# NOTE: IT IS VERY SIMILAR TO `train_model` THAT YOU HAVE ALREADY IMPLEMENTED EXCEPT, THE 'VALIDATION' PARTS ARE CHANGED TO 'TEST'\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_model(num_epochs: int, batch_size: int, learning_rate: float, validation_every_x_step: int, W_initial_weights: float) -> float:\n",
        "    \"\"\"\n",
        "    Trains the logistic regression model\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Number of epochs to train the model for\n",
        "        batch_size (int): Size of the mini-batch\n",
        "        learning_rate (float): Step size for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): Perform validation at every x-th step\n",
        "        W_initial_weights (float): Randomly initialized weight matrix\n",
        "\n",
        "    Returns:\n",
        "        train_loss: list containing training losses at each epoch\n",
        "        train_accuracy: list containing training accuracies at each epoch\n",
        "        train_step:\n",
        "        test_loss: list containing test losses at each epoch\n",
        "        test_accuracy: list containing test accuracies at each epoch\n",
        "        test_step:\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    train_step = []\n",
        "    test_loss = []\n",
        "    test_accuracy = []\n",
        "    test_step = []\n",
        "    epoch_last_step = 0\n",
        "\n",
        "    model = LogisticRegressionModel(W_initial_weights)\n",
        "\n",
        "    # note here that we have just replaced the validation set with the test set,\n",
        "    # the rest of the procedure remains the same\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        epoch_train_loss, epoch_train_accuracy, epoch_train_step, \\\n",
        "             epoch_test_loss, epoch_test_accuracy, epoch_test_step = \\\n",
        "                train_one_epoch(model, train_set, train_labels, test_set,\n",
        "                    test_labels, batch_size, learning_rate,\n",
        "                    validation_every_x_step)\n",
        "\n",
        "        train_loss += epoch_train_loss\n",
        "        train_accuracy += epoch_train_accuracy\n",
        "        train_step += [step + epoch_last_step for step in epoch_train_step]\n",
        "\n",
        "        test_loss += epoch_test_loss\n",
        "        test_accuracy += epoch_test_accuracy\n",
        "        test_step += [step + epoch_last_step for step in epoch_test_step]\n",
        "\n",
        "        epoch_last_step = train_step[-1]\n",
        "\n",
        "    return train_loss, train_accuracy, train_step, test_loss, test_accuracy, test_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:56:28.343576Z",
          "start_time": "2023-09-25T19:55:44.070135Z"
        },
        "id": "JGMi8PO5F8LM"
      },
      "outputs": [],
      "source": [
        "best_batch_size = 100\n",
        "best_lr = 0.1\n",
        "\n",
        "# Test the model!\n",
        "_ , _ , _ , \\\n",
        "     test_loss, test_accuracy, test_step = test_model(num_epochs=100, batch_size=best_batch_size,\n",
        "                                                        learning_rate=best_lr, validation_every_x_step=100, W_initial_weights=W_initial_weights)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(test_step, test_accuracy, label=f\"bs={best_batch_size}, lr={best_lr}\")\n",
        "plt.title('Test Accuracy')\n",
        "plt.ylim(0.6, 0.95)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best test accuracy: {test_accuracy[-1] * 100.0} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "X-lNJPHr-QmY"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.9 (1 point):** Report the test accuracy obtained by the Logistic regression. (1 sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "G7O5iio7-QmY"
      },
      "source": [
        "**Answer 3.9:** The test accuracy obtained is .... %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "GdwbfzRI1JKu"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 4. Comparing kNN, GNB and Logistic Regression Classifiers (5 points)\n",
        "\n",
        "Nice work! Now that your logistic regression classifier works, we will compare the performance of these 3 classifiers together. We will also compare the performance of above classifiers on FashionMNIST dataset.\n",
        "\n",
        "Fashion MNIST typically acts a drop-in replacement for the well-known MNIST dataset. Its structure is quite similar to MNIST except the labels now represent fashion categories (instead of digits). Here's a table representing each category:\n",
        "\n",
        "| Label      | Description |\n",
        "| ----------- | ----------- |\n",
        "| 0      | T-shirt/Top       |\n",
        "| 1   | Trouser        |\n",
        "| 2   | Pullover        |\n",
        "| 3   | Dress        |\n",
        "| 4   | Coat        |\n",
        "| 5   | Sandal        |\n",
        "| 6   | Shirt        |\n",
        "| 7   | Sneaker        |\n",
        "| 8   | Bag        |\n",
        "| 9   | Ankle Boot        |\n",
        "It is always important to visualize the data. In the next cell, we load the FashionMNIST dataset and plot 10 images from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwwa38ip1JKu"
      },
      "outputs": [],
      "source": [
        "def plot_fashion_mnist(images, labels, n_row=2, n_col=5):\n",
        "  fig, axes = plt.subplots(n_row, n_col, figsize=(10, 5))\n",
        "  for i in range(n_row * n_col):\n",
        "      ax = axes[i//n_col, i%n_col]\n",
        "      ax.imshow(images[i], cmap='gray')\n",
        "      ax.set_title('Label: {}'.format(labels[i]))\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLwMLcJQ1JKu"
      },
      "outputs": [],
      "source": [
        "# Load Fashion MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Shuffle the training set\n",
        "permuted = np.random.permutation(len(x_train))\n",
        "x_train, y_train = x_train[permuted], y_train[permuted]\n",
        "\n",
        "print(f\"Number of images for training: {x_train.shape[0]}\")\n",
        "print(f\"Number of images for testing: {x_test.shape[0]}\")\n",
        "print(f\"Size of Fashion MNIST images: {x_train[0].shape}\")\n",
        "\n",
        "# Visualize 10 images from the training set\n",
        "plot_fashion_mnist(images=x_train[:10], labels=y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yh1vDqa31JKu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pyzKCZF51JKw"
      },
      "source": [
        "Now, normalize the training and test data with the function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T22:31:01.403353Z",
          "start_time": "2023-09-25T22:31:01.289381Z"
        },
        "id": "hzphEN8c1JKw"
      },
      "outputs": [],
      "source": [
        "# NOTE: We normalize the training data by dividing the input by max value.\n",
        "max_value = np.max(x_train)\n",
        "\n",
        "train_images = x_train / max_value\n",
        "test_images = x_test / max_value\n",
        "\n",
        "# Vectorize the data\n",
        "train_images = train_images.reshape((train_images.shape[0], -1))\n",
        "test_images = test_images.reshape((test_images.shape[0], -1))\n",
        "\n",
        "print(f\"Training inputs' shape after vectorization: {train_images.shape}\")\n",
        "print(f\"Testing inputs' shape after vectorization: {test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qFhH7L1D1JKw"
      },
      "source": [
        "Now, using these functions, we will run the k-NN classifier on a subset of the FashionMNIST dataset! Particularly, we will use 50,000 samples for training, 10,000 samples for validation, and 10,000 samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0419N4rX1JKw"
      },
      "outputs": [],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7-JvQzvh1JKw"
      },
      "source": [
        "#### Test all three classifiers on FashionMNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sccvd6bm1JKw"
      },
      "outputs": [],
      "source": [
        "# kNN classifier (let's use the same best_k value we obtained for MNIST dataset)\n",
        "knn_test_accuracy = knn_classifier(train_set, train_labels, test_set, test_labels, k=best_k)\n",
        "print(f\"\\nkNN classifier: {knn_test_accuracy} %\")\n",
        "\n",
        "\n",
        "# GNB classifier\n",
        "gnb_test_accuracy = gnb_classifier(train_set, train_labels, test_set, test_labels)\n",
        "print(f\"\\nCNB classifier: {gnb_test_accuracy} %\")\n",
        "\n",
        "# Logistic regression\n",
        "_, _, _,  _, log_test_accuracy, _ = test_model(num_epochs=100, batch_size=100,\n",
        "                                             learning_rate=0.1, validation_every_x_step=100,\n",
        "                                             W_initial_weights=W_initial_weights)\n",
        "print(f\"\\nLogistic regression: {log_test_accuracy[-1] * 100.0} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "81Q5fxlA1JKw"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 4.1:** Report the test accuracies obtained using all the three classifiers on both MNIST and FashionMNIST results. Which classifier performs the best overall? Why do you think this is the case? (Justify in 3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gD6f6CNY1JKx"
      },
      "source": [
        "**Answer 4.1:**\n",
        "kNN accuracy = .... %, GNB accuracy = .... %, and Logistic regression accuracy = .... % (with batch_size = ...., lr = ....)\n",
        "\n",
        "Out of the three, ..... classifier performs the best.\n",
        "\n",
        "This is because ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "kSyOp8qh-QmZ"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## **Poission Regression (15 points)**\n",
        "\n",
        "In logistic regression, the assumption was that the binary label $Y_i \\in \\{0,1\\}$ followed a Bernoulli distribution, in other words $\\Pr(Y_i = 1 | X_i) = p_i$, where $p_i$ is the mean of the distribution. Under the independence assumption, the log-likelihood function could be derived as being\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^n (1-y_i) \\log(1-p_i) + y_i \\log(p_i).\n",
        "\\end{align}\n",
        "Then using the logit transformation, the parameterization we achieved was:\n",
        "\\begin{align}\n",
        "\\log\\frac{p_i}{1-p_i} = {\\bf x}_i . {\\bf w} + b, \\quad \\mbox{ or equivalently } \\quad p_i = \\frac{1}{1+\\exp(-{\\bf x}_i . {\\bf w} - b)}.\n",
        "\\end{align}\n",
        "Furthemore, the weight vector and bias could both be found through maximizing the log-likelihood function.\n",
        "\n",
        "The following problem attempts to generalize this to the case where $Y_i \\in \\mathbb{N}$, meaning that the outcome could be any natural number.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "aO5IJxRb-QmZ"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.1 (5 points)**\n",
        "\n",
        "Assume that each $Y_i \\in \\mathbb{N}$ follows the Poisson distribution (with mean $\\mu_i \\geq 0$):\n",
        "\t\\begin{align}\n",
        "\t\\Pr(Y_i = k | X_i) = \\frac{\\mu_i^k}{k!} \\exp(-\\mu_i), ~~ k = 0, 1, 2, \\ldots\n",
        "\t\\end{align}\n",
        "Given a dataset $\\mathcal{D} = \\{{\\bf x}_i, y_i\\}_{i=1}^n$, what is the log-likelihood function (of $\\mu_i$'s) given $\\mathcal{D}$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aSoEPKdY-QmZ"
      },
      "source": [
        "**Answer 5.1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "6M7SnE9D-QmZ"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.2 (2 points)**\n",
        "Can you give some justification of the parameterization below?\n",
        "\t\\begin{align}\n",
        "\t\\log\\mu_i = {\\bf x}_i . {\\bf w} + b.\n",
        "\t\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "t3azpf3Q-Qma"
      },
      "source": [
        "**Answer 5.2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "0HS7XVAN-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.3 (3 points)**\n",
        "Using the solution to the above, write down the objective function for Poisson regression. Please specify the optimization variables and whether you are maximizing or minimizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BaH2xvDq-Qma"
      },
      "source": [
        "**Answer 5.3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "5_4CdYOt-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.4 (5 points)**\n",
        "Compute the gradient of your objective function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c9fxn2EP-Qma"
      },
      "source": [
        "**Answer 5.4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "vVnIKpdY-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
